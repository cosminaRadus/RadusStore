import os
import io
import pandas as pd
from airflow.models import BaseOperator
from airflow.utils.decorators import apply_defaults
from airflow.models import Variable
from google.cloud import bigquery
from google.oauth2 import service_account
from google.cloud import storage


class GcsToBigQueryOperator(BaseOperator):
    @apply_defaults
    def __init__(self, credentials_dict, bucket_name, gsc_file_name, dataset_id, table_id, *args, **kwargs):
        super(GcsToBigQueryOperator, self).__init__(*args, **kwargs)
        self.credentials_dict = credentials_dict
        self.bucket_name = bucket_name
        self.gsc_file_name = gsc_file_name
        self.dataset_id = dataset_id
        self.table_id = table_id

    # def is_valid_row(self, row):
    #     pattern = re.compile(r'^[a-zA-Z0-9\s.,;:?!-]*$')
    #     return all(pattern.match(str(cell)) for cell in row)

               
    def execute(self, context):
        self.log.info("Executing GcsToBigQueryOperator...")
        try:
            credentials = service_account.Credentials.from_service_account_info(self.credentials_dict)

            client = bigquery.Client(credentials=credentials)
            storage_client = storage.Client(credentials=credentials)
            table_ref = client.dataset(self.dataset_id).table(self.table_id)
      
            bucket = storage_client.bucket(self.bucket_name)
         
            blob = bucket.blob(self.gsc_file_name)
           
            self.log.info(f"Blob exists: {bucket.blob(self.gsc_file_name).exists()}")
            if blob.exists(timeout=15):

                self.log.info('Blob_exists')
                csv_data = blob.download_as_text()
            
                # we skip the bad rows generated by the api where a string contains a comma and
                # gets interpreted as a separated value in a csv,
                # resulting in more columns than expected
                df = pd.read_csv(io.StringIO(csv_data), on_bad_lines='skip')
        
                if df.iloc[:, 0].duplicated().any():
                    df.iloc[:, 0] = range(1, len(df) + 1)
            
                modified_csv = df.to_csv(index=False)
                modified_blob = bucket.blob(self.gsc_file_name)
                modified_blob.upload_from_string(modified_csv)
        
                uri = f"gs://{self.bucket_name}/{self.gsc_file_name}"
            
                job_config = bigquery.LoadJobConfig(
                    source_format=bigquery.SourceFormat.CSV,
                    skip_leading_rows=1,  # skip the header
                    autodetect=False,
                    write_disposition="WRITE_TRUNCATE"
                )

                load_job = client.load_table_from_uri(uri, table_ref, job_config=job_config)
                load_job.result(timeout=60) 
                self.log.info(f"Loaded {load_job.output_rows} rows into {table_ref}.")
            else:
                self.log.info("Blob doesn't exist")
        except Exception as e:
            self.log.info(f"Error in GcsToBigQueryOperator: {e}")

    